{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Berlin City-Wide Emissions (2023) \u2014 Exploratory Data Review\n",
        "\n",
        "This notebook provides a structured first look at the dataset in `CSV data/2023_City_Wide_Emissions_Berlin.csv`.\n",
        "It covers loading, inspecting schema, missing values, unique values in key columns, summary statistics, duplicates,\n",
        "categorical vs numerical identification, and a couple of simple visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports\n",
        "We import common libraries for data analysis. `pandas` for dataframes, `numpy` for numerics, and `seaborn/matplotlib` for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "\n",
        "# Display options for readability\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.width', 120)\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "print('pandas:', pd.__version__)\n",
        "print('numpy :', np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the CSV and preview rows\n",
        "We construct the path and read the CSV. Then we preview the first few rows to get a quick sense of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the CSV (relative to this notebook)\n",
        "csv_path = Path('CSV data') / '2023_City_Wide_Emissions_Berlin.csv'\n",
        "print('Loading from:', csv_path)\n",
        "\n",
        "# Read CSV; adjust encoding/errors if needed for special characters\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Show the first 10 rows (adjust to 5 if you prefer)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Columns and data types\n",
        "We list all columns and their inferred data types (`dtypes`).\n",
        "This helps identify text (object), numeric, datetime, and categorical-like fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Columns ({}):'.format(len(df.columns)))\n",
        "print(list(df.columns))\n",
        "\n",
        "# Show data types in a tidy table\n",
        "dtype_table = df.dtypes.to_frame(name='dtype')\n",
        "dtype_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Shape (rows and columns)\n",
        "We check the overall size of the dataset as `(n_rows, n_columns)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Shape:', df.shape)\n",
        "print('Number of rows   :', df.shape[0])\n",
        "print('Number of columns:', df.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5\u20136. Missing values per column and total\n",
        "We compute missing (null) counts per column and overall, including percentage per column to gauge severity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_counts = df.isna().sum().sort_values(ascending=False)\n",
        "null_pct = (df.isna().mean() * 100).round(2)\n",
        "missing_summary = pd.concat([null_counts, null_pct], axis=1)\n",
        "missing_summary.columns = ['null_count', 'null_pct']\n",
        "missing_summary\n",
        "\n",
        "print('Total null values in dataframe:', int(null_counts.sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Unique values in key columns\n",
        "We attempt to locate columns commonly used in emissions datasets (e.g., city, country, year, scope, emission type)\n",
        "and display their unique values. If a column is not present, we skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper to find columns by candidate names (case-insensitive substring match)\n",
        "def find_columns(candidates):\n",
        "    cols_lower = {c.lower(): c for c in df.columns}  # map lower->actual\n",
        "    found = []\n",
        "    for cand in candidates:\n",
        "        # direct exact lower-case match\n",
        "        if cand in cols_lower:\n",
        "            found.append(cols_lower[cand])\n",
        "            continue\n",
        "        # substring search across columns\n",
        "        for c in df.columns:\n",
        "            if cand in c.lower():\n",
        "                found.append(c)\n",
        "    # de-duplicate while preserving order\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for c in found:\n",
        "        if c not in seen:\n",
        "            uniq.append(c)\n",
        "            seen.add(c)\n",
        "    return uniq\n",
        "\n",
        "key_groups = {\n",
        "    'city': ['city', 'municipality', 'borough'],\n",
        "    'country': ['country', 'nation'],\n",
        "    'year': ['year', 'reporting year', 'report_year', 'fiscal year'],\n",
        "    'scope': ['scope', 'emission scope', 'ghg scope'],\n",
        "    'emission_type': ['emission type', 'emissions type', 'type', 'category', 'sector']\n",
        "}\n",
        "\n",
        "for key, cands in key_groups.items():\n",
        "    found = find_columns([c.lower() for c in cands])\n",
        "    if not found:\n",
        "        print(f\"No matching column found for '{key}'. Candidates tried: {cands}\")\n",
        "        continue\n",
        "    for col in found:\n",
        "        try:\n",
        "            n_unique = df[col].nunique(dropna=True)\n",
        "        except TypeError:\n",
        "            # Fallback for columns with unhashable entries (lists/dicts)\n",
        "            n_unique = int(df[col].dropna().shape[0])\n",
        "\n",
        "        print(f\"\\n\u2014 {key.upper()} \u2014 column: '{col}' (unique: {n_unique})\")\n",
        "\n",
        "        try:\n",
        "            raw_uniques = df[col].dropna().unique()\n",
        "        except TypeError:\n",
        "            # If unique() fails (e.g., due to unhashable types), coerce to string\n",
        "            raw_uniques = df[col].dropna().astype(str).unique()\n",
        "\n",
        "        values = list(raw_uniques)\n",
        "        try:\n",
        "            values = sorted(values)\n",
        "        except TypeError:\n",
        "            try:\n",
        "                values = sorted(values, key=lambda x: str(x))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        preview = pd.Series(values[:20], name=f\"unique_{col}\")\n",
        "        display(preview.to_frame())\n",
        "\n",
        "        if n_unique > 20:\n",
        "            print('... (truncated)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary statistics (numeric columns)\n",
        "We compute standard descriptive statistics (count, mean, std, min, quartiles, max) for all numeric columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_summary = df.describe(include=[np.number]).T\n",
        "numeric_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Duplicate rows\n",
        "We check how many rows are exact duplicates. If needed, we could display them, but we start by reporting the count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dup_count = df.duplicated().sum()\n",
        "print('Duplicate rows:', int(dup_count))\n",
        "\n",
        "# If you want to see the duplicate rows, uncomment:\n",
        "# df[df.duplicated(keep=False)].sort_values(list(df.columns)).head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Categorical vs numerical columns\n",
        "We infer likely categorical vs numerical columns using the pandas dtypes and simple heuristics (e.g., low-cardinality numerics might be categorical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base detection via dtypes\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Heuristic: low-cardinality numerics might be categorical (e.g., 0/1 flags, enums)\n",
        "low_card_numeric = []\n",
        "for col in numeric_cols:\n",
        "    nunique = df[col].nunique(dropna=True)\n",
        "    if 1 < nunique <= max(10, int(0.02 * len(df))):\n",
        "        low_card_numeric.append(col)\n",
        "\n",
        "print('Categorical-like columns:')\n",
        "print(sorted(list(set(categorical_cols + low_card_numeric))))\n",
        "\n",
        "print('\nNumerical columns:')\n",
        "print(sorted(numeric_cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Simple visuals\n",
        "We create a couple of basic plots, if the necessary columns exist:\n",
        "- Countplot for an \"emission type\"-like column.\n",
        "- Bar chart of total emissions by year (using a best-effort guess for the emissions column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to detect an 'emissions type' column\n",
        "em_type_candidates = ['emission type', 'emissions type', 'type', 'category', 'sector']\n",
        "em_type_cols = []\n",
        "for cand in em_type_candidates:\n",
        "    for c in df.columns:\n",
        "        if cand in c.lower():\n",
        "            em_type_cols.append(c)\n",
        "em_type_cols = list(dict.fromkeys(em_type_cols))  # de-dupe preserve order\n",
        "\n",
        "if em_type_cols:\n",
        "    col = em_type_cols[0]\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.countplot(data=df, x=col, order=df[col].value_counts().index, color='#4C72B0')\n",
        "    plt.title(f'Count of {col}')\n",
        "    plt.xticks(rotation=30, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No emission-type-like column found for countplot.')\n",
        "\n",
        "# Try to detect a 'year' column\n",
        "year_cols = []\n",
        "for cand in ['year', 'reporting year', 'report_year', 'fiscal year']:\n",
        "    for c in df.columns:\n",
        "        if cand in c.lower():\n",
        "            year_cols.append(c)\n",
        "year_cols = list(dict.fromkeys(year_cols))\n",
        "\n",
        "# Try to detect a numeric emissions measure column\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "em_value_cols = [c for c in num_cols if any(k in c.lower() for k in ['emission', 'co2', 'co\u2082', 'ghg', 'tco2e', 'ton', 'kt', 'mt', 'value', 'quantity', 'total'])]\n",
        "\n",
        "if year_cols and em_value_cols:\n",
        "    ycol = year_cols[0]\n",
        "    vcol = em_value_cols[0]\n",
        "    print(f'Using year column: {ycol} | emissions column: {vcol}')\n",
        "    # Aggregate total emissions by year\n",
        "    by_year = (\n",
        "        df.groupby(ycol, dropna=True)[vcol]\n",
        "          .sum()\n",
        "          .reset_index()\n",
        "          .sort_values(ycol)\n",
        "    )\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(data=by_year, x=ycol, y=vcol, color='#55A868')\n",
        "    plt.title(f'Total {vcol} by {ycol}')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    if not year_cols:\n",
        "        print('No year-like column found; skipping emissions-by-year chart.')\n",
        "    if not em_value_cols:\n",
        "        print('No numeric emissions column detected; skipping emissions-by-year chart.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Notes:\n",
        "- If column names differ from the guesses above, adjust the candidate lists accordingly.\n",
        "- For more plots (e.g., breakdown by sector/scope/gas), you can extend the detection logic or set column names explicitly."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}